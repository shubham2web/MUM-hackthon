"""
Web Scraper Tool - Intelligent External RAG Implementation

V2 UPGRADE: Now includes caching and AI summarization for token efficiency.

Features:
- Smart caching: 24-hour cache prevents redundant fetches
- AI summarization: Compresses 20KB HTML into 2KB factual summaries
- Token savings: ~90% reduction in context window usage
- Speed: Cache hits return in ~0.01s vs ~10s for live fetches

This solves the problem where LLMs generate stories based on URL text
(e.g., seeing "johannesburg" and assuming BRICS instead of reading actual G20 content).
"""
from __future__ import annotations

import re
import json
import os
import time
import logging
from typing import Optional

try:
    import requests
    from bs4 import BeautifulSoup
    DEPS_AVAILABLE = True
except ImportError:
    DEPS_AVAILABLE = False


logger = logging.getLogger(__name__)

# Configuration
CACHE_FILE = os.path.join(os.path.dirname(__file__), '..', 'data', 'web_cache.json')
CACHE_EXPIRY = 3600 * 24  # 24 hours
MAX_SUMMARY_LENGTH = 3000  # Input length for summarizer (chars)


def load_cache() -> dict:
    """
    Load cached URL summaries from JSON file.
    
    Returns:
        Dictionary of cached URL data
    """
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load cache: {e}")
            return {}
    return {}


def save_cache(cache_data: dict) -> None:
    """
    Save cache to JSON file.
    
    Args:
        cache_data: Dictionary of URL cache entries
    """
    try:
        os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Failed to save cache: {e}")


def summarize_content(text: str) -> str:
    """
    Uses a fast LLM to compress raw web text into a factual summary.
    
    This is the "Summary Layer" that reduces token usage by ~90%.
    Instead of dumping 8,000 characters of raw HTML into the context,
    we compress it into a dense, factual summary.
    
    Args:
        text: Raw web content to summarize
        
    Returns:
        Compressed summary (under 300 words)
        
    Examples:
        >>> raw = "Navigation Home About Us... Article Title: G20 Summit... 20KB of HTML..."
        >>> summary = summarize_content(raw)
        >>> print(summary)
        'G20 Summit held in New Delhi on Sept 2023. Leaders from 20 nations...'
    """
    try:
        from core.ai_agent import AiAgent
        # Use "llama3" which maps to llama-3.1-8b-instant on Groq (fast/cheap model)
        llm = AiAgent(model_name="llama3")
        
        prompt = f"""You are a research assistant. Summarize the following web content.

RULES:
1. Focus on hard facts, numbers, dates, and key entities.
2. Ignore navigation menus, ads, and boilerplate text.
3. Keep it under 300 words.
4. Output ONLY the summary.

CONTENT:
{text[:MAX_SUMMARY_LENGTH]}"""
        
        # Use call_blocking method with correct signature
        response = llm.call_blocking(
            user_message=prompt,
            system_prompt="You are a concise research assistant that summarizes web content accurately.",
            max_tokens=400
        )
        
        summary = response.text
        logger.info(f"Generated summary: {len(summary)} chars from {len(text)} chars original")
        return summary
        
    except Exception as e:
        # Fallback: Return truncated raw text if summarization fails
        logger.warning(f"Summarization failed: {e}. Using fallback truncation.")
        return f"[SUMMARY UNAVAILABLE] Raw text fragment: {text[:500]}..."


# Configuration
CACHE_FILE = os.path.join(os.path.dirname(__file__), '..', 'data', 'web_cache.json')
CACHE_EXPIRY = 3600 * 24  # 24 hours
MAX_SUMMARY_LENGTH = 3000  # Input length for summarizer (chars)


def extract_url(text: str) -> Optional[str]:
    """
    Extract the first URL from text using regex.
    
    Args:
        text: Text that may contain URLs
        
    Returns:
        First URL found, or None
        
    Examples:
        >>> extract_url("Check this http://example.com link")
        'http://example.com'
        >>> extract_url("Visit https://news.site/article?id=123")
        'https://news.site/article?id=123'
    """
    # Regex pattern for URLs (http/https)
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    
    match = re.search(url_pattern, text)
    if match:
        return match.group(0)
    return None


def fetch_url_content(url: str, max_length: int = 8000, timeout: int = 10, force_refresh: bool = False) -> str:
    """
    Smart fetch with caching and summarization.
    
    UPGRADE V2 FLOW:
    1. CHECK CACHE: If URL was fetched in last 24 hours, return cached summary (~0.01s)
    2. FETCH LIVE: If cache miss, download the webpage (~5-10s)
    3. SUMMARIZE: Compress with LLM from 20KB â†’ 2KB (~2-3s)
    4. SAVE CACHE: Store for future requests
    
    This prevents URL-keyword hallucination by reading actual content,
    while also saving tokens and reducing latency.
    
    The Problem It Solves:
    - Without this: LLM sees "...g20...johannesburg..." in URL and guesses content
    - With this: LLM reads actual webpage text (G20 summit in New Delhi, not BRICS)
    - V2 Bonus: Cached responses are instant and free
    
    Args:
        url: The URL to fetch
        max_length: Maximum text length for raw content (before summarization)
        timeout: Request timeout in seconds
        force_refresh: Skip cache and fetch fresh content
        
    Returns:
        "[CACHED SUMMARY]" or "[LIVE FETCH]" prefix + content/summary
        
    Examples:
        >>> # First call: Live fetch + summarize (~10s)
        >>> content = fetch_url_content("https://news.site/article")
        >>> print(content)
        '[LIVE FETCH] G20 Summit held in New Delhi...'
        
        >>> # Second call: Cache hit (~0.01s, zero tokens)
        >>> content = fetch_url_content("https://news.site/article")
        >>> print(content)
        '[CACHED SUMMARY] G20 Summit held in New Delhi...'
    """
    if not DEPS_AVAILABLE:
        error_msg = (
            "Web scraping dependencies not installed. "
            "Run: pip install requests beautifulsoup4"
        )
        logger.error(error_msg)
        return f"Error: {error_msg}"
    
    # 1. CHECK CACHE
    cache = load_cache()
    if not force_refresh and url in cache:
        cached_entry = cache[url]
        age = time.time() - cached_entry['timestamp']
        
        if age < CACHE_EXPIRY:
            logger.info(f"ðŸš€ Cache Hit for {url} (age: {age/3600:.1f}h)")
            return f"[CACHED SUMMARY] {cached_entry['summary']}"
        else:
            logger.info(f"Cache expired for {url} (age: {age/3600:.1f}h > 24h)")
    
    # 2. FETCH LIVE CONTENT (Cache Miss)
    logger.info(f"ðŸŒ Fetching live: {url}")
    
    try:
        # Set realistic user agent to avoid bot blocking
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; ATLAS-Bot/1.0)'
        }
        
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Remove script, style, navigation, ads - they contain no readable content
        for script in soup(["script", "style", "noscript", "nav", "footer", "header"]):
            script.extract()
        
        # Get cleaned text
        raw_text = soup.get_text(separator=' ', strip=True)
        
        # Basic cleanup
        lines = (line.strip() for line in raw_text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        raw_text = ' '.join(chunk for chunk in chunks if chunk)
        
        logger.info(f"Fetched {len(raw_text)} characters from {url}")
        
        # 3. SUMMARIZE
        summary = summarize_content(raw_text)
        
        # 4. SAVE TO CACHE
        cache[url] = {
            'summary': summary,
            'timestamp': time.time(),
            'original_length': len(raw_text),
            'summary_length': len(summary)
        }
        save_cache(cache)
        
        logger.info(f"Cached summary for {url} (compression: {len(raw_text)} â†’ {len(summary)} chars)")
        return f"[LIVE FETCH] {summary}"
        
    except requests.exceptions.Timeout:
        error_msg = f"Timeout: URL took longer than {timeout} seconds to respond"
        logger.warning(error_msg)
        return f"Error fetching URL: {error_msg}"
        
    except requests.exceptions.RequestException as e:
        error_msg = f"Network error: {str(e)}"
        logger.warning(error_msg)
        return f"Error fetching URL: {error_msg}"
        
    except Exception as e:
        error_msg = f"Unexpected error: {str(e)}"
        logger.error(error_msg)
        return f"Error fetching URL: {error_msg}"


def clear_cache(url: Optional[str] = None) -> int:
    """
    Clear cache entries.
    
    Args:
        url: Specific URL to clear, or None to clear all
        
    Returns:
        Number of entries cleared
        
    Examples:
        >>> # Clear specific URL
        >>> cleared = clear_cache("https://example.com")
        >>> print(f"Cleared {cleared} entries")
        
        >>> # Clear all cache
        >>> cleared = clear_cache()
        >>> print(f"Cleared {cleared} entries")
    """
    cache = load_cache()
    
    if url is None:
        # Clear all
        count = len(cache)
        save_cache({})
        logger.info(f"Cleared entire cache ({count} entries)")
        return count
    else:
        # Clear specific URL
        if url in cache:
            del cache[url]
            save_cache(cache)
            logger.info(f"Cleared cache for {url}")
            return 1
        return 0


def get_cache_stats() -> dict:
    """
    Get cache statistics.
    
    Returns:
        Dictionary with cache metrics
        
    Examples:
        >>> stats = get_cache_stats()
        >>> print(f"Total cached URLs: {stats['total_urls']}")
        >>> print(f"Total savings: {stats['total_savings_kb']} KB")
    """
    cache = load_cache()
    
    total_original = 0
    total_summary = 0
    expired = 0
    current_time = time.time()
    
    for entry in cache.values():
        total_original += entry.get('original_length', 0)
        total_summary += entry.get('summary_length', 0)
        
        age = current_time - entry.get('timestamp', 0)
        if age > CACHE_EXPIRY:
            expired += 1
    
    return {
        'total_urls': len(cache),
        'expired_urls': expired,
        'total_original_kb': total_original / 1024,
        'total_summary_kb': total_summary / 1024,
        'total_savings_kb': (total_original - total_summary) / 1024,
        'compression_ratio': f"{(total_summary / total_original * 100) if total_original else 0:.1f}%"
    }


# Test function for development
def _test_smart_scraper():
    """Test the smart scraper with timing"""
    test_url = "https://example.com"
    
    print("=" * 80)
    print("SMART WEB SCRAPER TEST")
    print("=" * 80)
    
    print(f"\n--- Run 1: Live Fetch & Summarize ---")
    start = time.time()
    result1 = fetch_url_content(test_url)
    time1 = time.time() - start
    print(f"Result: {result1[:200]}...")
    print(f"â±ï¸  Time taken: {time1:.2f}s")
    
    print(f"\n--- Run 2: Cache Hit ---")
    start = time.time()
    result2 = fetch_url_content(test_url)
    time2 = time.time() - start
    print(f"Result: {result2[:200]}...")
    print(f"â±ï¸  Time taken: {time2:.2f}s")
    
    print(f"\n--- Cache Stats ---")
    stats = get_cache_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    print(f"\n--- Speed Improvement ---")
    if time2 > 0:
        speedup = time1 / time2
        print(f"Cache is {speedup:.0f}x faster!")
        print(f"Saved {time1 - time2:.2f} seconds")
    
    print("=" * 80)


if __name__ == "__main__":
    # Run test if executed directly
    _test_smart_scraper()
