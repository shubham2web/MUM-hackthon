{
  "summary": {
    "total_tests": 13,
    "passed": 0,
    "pass_rate": 0.0,
    "avg_precision": 0.5256410256410257,
    "avg_recall": 0.8461538461538461,
    "avg_f1": 0.6128205128205129,
    "avg_relevance_score": 0.46284301099612557,
    "target_met": false
  },
  "individual_results": [
    {
      "test": "Exact Turn Recall",
      "description": "Tests ability to retrieve exact turn content when explicitly requested",
      "metrics": {
        "precision": 0.3333333333333333,
        "recall": 1.0,
        "f1_score": 0.5,
        "relevance_score": 0.47252607066160707,
        "retrieved_count": 3,
        "expected_count": 1,
        "true_positives": 1
      },
      "passed": false,
      "query": "What did the proponent say about safety in turn 1?",
      "expected_count": 1,
      "retrieved_count": 3
    },
    {
      "test": "Topic-Based Retrieval",
      "description": "Tests semantic search for topic-related arguments across multiple turns",
      "metrics": {
        "precision": 1.0,
        "recall": 1.0,
        "f1_score": 1.0,
        "relevance_score": 0.5763524691186338,
        "retrieved_count": 2,
        "expected_count": 2,
        "true_positives": 2
      },
      "passed": false,
      "query": "Arguments about economic costs and affordability",
      "expected_count": 2,
      "retrieved_count": 2
    },
    {
      "test": "Role Filtering",
      "description": "Tests ability to filter memories by role metadata",
      "metrics": {
        "precision": 0.6666666666666666,
        "recall": 1.0,
        "f1_score": 0.8,
        "relevance_score": 0.5123124362884873,
        "retrieved_count": 3,
        "expected_count": 2,
        "true_positives": 2
      },
      "passed": false,
      "query": "What environmental arguments did the opponent make?",
      "expected_count": 2,
      "retrieved_count": 3
    },
    {
      "test": "Recent Context Retrieval",
      "description": "Tests ability to prioritize recent context over older content",
      "metrics": {
        "precision": 1.0,
        "recall": 0.5,
        "f1_score": 0.6666666666666666,
        "relevance_score": 0.3140793007851119,
        "retrieved_count": 1,
        "expected_count": 2,
        "true_positives": 1
      },
      "passed": false,
      "query": "Most recent arguments in the debate about modern technology",
      "expected_count": 2,
      "retrieved_count": 1
    },
    {
      "test": "Irrelevant Query Handling",
      "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0.0,
        "relevance_score": 0.0,
        "retrieved_count": 2,
        "expected_count": 0,
        "true_positives": 0
      },
      "passed": false,
      "query": "What was said about cryptocurrency mining and blockchain technology?",
      "expected_count": 0,
      "retrieved_count": 2
    },
    {
      "test": "Role Reversal - Original Stance Retrieval",
      "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
      "metrics": {
        "precision": 0.3333333333333333,
        "recall": 0.5,
        "f1_score": 0.4,
        "relevance_score": 0.5074717803584758,
        "retrieved_count": 3,
        "expected_count": 2,
        "true_positives": 1
      },
      "passed": false,
      "query": "What was my original argument FOR solar energy that I now need to critique?",
      "expected_count": 2,
      "retrieved_count": 3
    },
    {
      "test": "Role Reversal - Adopt Opponent's Position",
      "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
      "metrics": {
        "precision": 0.6666666666666666,
        "recall": 1.0,
        "f1_score": 0.8,
        "relevance_score": 0.37283811450215987,
        "retrieved_count": 3,
        "expected_count": 2,
        "true_positives": 2
      },
      "passed": false,
      "query": "What arguments against nuclear energy should I now adopt after role reversal?",
      "expected_count": 2,
      "retrieved_count": 3
    },
    {
      "test": "Multi-Turn Chat Context",
      "description": "Tests retrieval of previous explanations in multi-turn conversation",
      "metrics": {
        "precision": 0.3333333333333333,
        "recall": 1.0,
        "f1_score": 0.5,
        "relevance_score": 0.589671350560521,
        "retrieved_count": 3,
        "expected_count": 1,
        "true_positives": 1
      },
      "passed": false,
      "query": "What did you explain about entanglement earlier?",
      "expected_count": 1,
      "retrieved_count": 3
    },
    {
      "test": "Topic Switching",
      "description": "Tests ability to retrieve correct topic context after switching topics",
      "metrics": {
        "precision": 0.3333333333333333,
        "recall": 1.0,
        "f1_score": 0.5,
        "relevance_score": 0.5825337417535179,
        "retrieved_count": 3,
        "expected_count": 1,
        "true_positives": 1
      },
      "passed": false,
      "query": "Previous discussion about AI safety concerns and alignment",
      "expected_count": 1,
      "retrieved_count": 3
    },
    {
      "test": "OCR Context Recall",
      "description": "Tests retrieval of previous OCR fact-checking analyses",
      "metrics": {
        "precision": 0.5,
        "recall": 1.0,
        "f1_score": 0.6666666666666666,
        "relevance_score": 0.5734389587851851,
        "retrieved_count": 2,
        "expected_count": 1,
        "true_positives": 1
      },
      "passed": false,
      "query": "What did we find about vaccine misinformation earlier?",
      "expected_count": 1,
      "retrieved_count": 2
    },
    {
      "test": "Multi-Image Context",
      "description": "Tests ability to connect related misinformation across multiple images",
      "metrics": {
        "precision": 0.6666666666666666,
        "recall": 1.0,
        "f1_score": 0.8,
        "relevance_score": 0.49621988916497217,
        "retrieved_count": 3,
        "expected_count": 2,
        "true_positives": 2
      },
      "passed": false,
      "query": "Previous climate misinformation we analyzed",
      "expected_count": 2,
      "retrieved_count": 3
    },
    {
      "test": "Similar Content Disambiguation",
      "description": "Tests ability to distinguish between very similar statements",
      "metrics": {
        "precision": 0.5,
        "recall": 1.0,
        "f1_score": 0.6666666666666666,
        "relevance_score": 0.5331721115748315,
        "retrieved_count": 2,
        "expected_count": 1,
        "true_positives": 1
      },
      "passed": false,
      "query": "What was said about carbon emissions during plant operation specifically?",
      "expected_count": 1,
      "retrieved_count": 2
    },
    {
      "test": "Long-Term Memory Retention",
      "description": "Tests retrieval of early content after many subsequent memories",
      "metrics": {
        "precision": 0.5,
        "recall": 1.0,
        "f1_score": 0.6666666666666666,
        "relevance_score": 0.48634291939612906,
        "retrieved_count": 2,
        "expected_count": 1,
        "true_positives": 1
      },
      "passed": false,
      "query": "What was the opening statement about solar energy?",
      "expected_count": 1,
      "retrieved_count": 2
    }
  ],
  "metadata": {
    "execution_time": 2.114331,
    "start_time": "2025-11-11T06:11:59.424327",
    "end_time": "2025-11-11T06:12:01.538658",
    "memory_backend": "unknown",
    "embedding_model": "unknown"
  }
}