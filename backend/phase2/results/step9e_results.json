{
  "step": "9e-1",
  "description": "Percentile Threshold Sweep",
  "timestamp": "2025-11-11T17:54:56.749805",
  "thresholds_tested": [
    0.6,
    0.7,
    0.75,
    0.8,
    0.85,
    0.9
  ],
  "individual_results": [
    {
      "percentile": 0.6,
      "results": {
        "summary": {
          "total_tests": 13,
          "passed": 6,
          "pass_rate": 0.46153846153846156,
          "avg_precision": 0.3294871794871795,
          "avg_recall": 0.9230769230769231,
          "avg_f1": 0.4750915750915751,
          "avg_relevance_score": 0.7478316897328251,
          "target_met": false
        },
        "individual_results": [
          {
            "test": "Exact Turn Recall",
            "description": "Tests ability to retrieve exact turn content when explicitly requested",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.6925304452290909,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did the proponent say about safety in turn 1?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic-Based Retrieval",
            "description": "Tests semantic search for topic-related arguments across multiple turns",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6776253042486277,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Arguments about economic costs and affordability",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Filtering",
            "description": "Tests ability to filter memories by role metadata",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.7111510428902025,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What environmental arguments did the opponent make?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Recent Context Retrieval",
            "description": "Tests ability to prioritize recent context over older content",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.9905812431376531,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": true,
            "query": "Most recent arguments in the debate about modern technology",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Irrelevant Query Handling",
            "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
            "metrics": {
              "precision": 0.0,
              "recall": 0.0,
              "f1_score": 0.0,
              "relevance_score": 0.0,
              "retrieved_count": 2,
              "expected_count": 0,
              "true_positives": 0
            },
            "passed": false,
            "query": "What was said about cryptocurrency mining and blockchain technology?",
            "expected_count": 0,
            "retrieved_count": 2
          },
          {
            "test": "Role Reversal - Original Stance Retrieval",
            "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5150507670710887,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What was my original argument FOR solar energy that I now need to critique?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Reversal - Adopt Opponent's Position",
            "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6572698805898227,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What arguments against nuclear energy should I now adopt after role reversal?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Turn Chat Context",
            "description": "Tests retrieval of previous explanations in multi-turn conversation",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.653673191055405,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did you explain about entanglement earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic Switching",
            "description": "Tests ability to retrieve correct topic context after switching topics",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 1.0,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "Previous discussion about AI safety concerns and alignment",
            "expected_count": 1,
            "retrieved_count": 5
          },
          {
            "test": "OCR Context Recall",
            "description": "Tests retrieval of previous OCR fact-checking analyses",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 1.0,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What did we find about vaccine misinformation earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Image Context",
            "description": "Tests ability to connect related misinformation across multiple images",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.8239300923048358,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": true,
            "query": "Previous climate misinformation we analyzed",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Similar Content Disambiguation",
            "description": "Tests ability to distinguish between very similar statements",
            "metrics": {
              "precision": 0.3333333333333333,
              "recall": 1.0,
              "f1_score": 0.5,
              "relevance_score": 1.0,
              "retrieved_count": 3,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was said about carbon emissions during plant operation specifically?",
            "expected_count": 1,
            "retrieved_count": 3
          },
          {
            "test": "Long-Term Memory Retention",
            "description": "Tests retrieval of early content after many subsequent memories",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 1.0,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was the opening statement about solar energy?",
            "expected_count": 1,
            "retrieved_count": 5
          }
        ],
        "metadata": {
          "execution_time": 1.052361,
          "start_time": "2025-11-11T17:54:39.838774",
          "end_time": "2025-11-11T17:54:40.891135",
          "memory_backend": "unknown",
          "embedding_model": "unknown"
        }
      },
      "total_time": 6.648184299468994,
      "timestamp": "2025-11-11 17:54:40",
      "implementation_status": "baseline_only"
    },
    {
      "percentile": 0.7,
      "results": {
        "summary": {
          "total_tests": 13,
          "passed": 5,
          "pass_rate": 0.38461538461538464,
          "avg_precision": 0.3294871794871795,
          "avg_recall": 0.9230769230769231,
          "avg_f1": 0.4750915750915751,
          "avg_relevance_score": 0.7147362196272506,
          "target_met": false
        },
        "individual_results": [
          {
            "test": "Exact Turn Recall",
            "description": "Tests ability to retrieve exact turn content when explicitly requested",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.6409076439500262,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did the proponent say about safety in turn 1?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic-Based Retrieval",
            "description": "Tests semantic search for topic-related arguments across multiple turns",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6268164093935338,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Arguments about economic costs and affordability",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Filtering",
            "description": "Tests ability to filter memories by role metadata",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6575078618563417,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What environmental arguments did the opponent make?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Recent Context Retrieval",
            "description": "Tests ability to prioritize recent context over older content",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.9532804905469288,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": true,
            "query": "Most recent arguments in the debate about modern technology",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Irrelevant Query Handling",
            "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
            "metrics": {
              "precision": 0.0,
              "recall": 0.0,
              "f1_score": 0.0,
              "relevance_score": 0.0,
              "retrieved_count": 2,
              "expected_count": 0,
              "true_positives": 0
            },
            "passed": false,
            "query": "What was said about cryptocurrency mining and blockchain technology?",
            "expected_count": 0,
            "retrieved_count": 2
          },
          {
            "test": "Role Reversal - Original Stance Retrieval",
            "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5138987164134539,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What was my original argument FOR solar energy that I now need to critique?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Reversal - Adopt Opponent's Position",
            "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6067262562495053,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What arguments against nuclear energy should I now adopt after role reversal?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Turn Chat Context",
            "description": "Tests retrieval of previous explanations in multi-turn conversation",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.6031634234682046,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did you explain about entanglement earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic Switching",
            "description": "Tests ability to retrieve correct topic context after switching topics",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 0.9297084489869871,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "Previous discussion about AI safety concerns and alignment",
            "expected_count": 1,
            "retrieved_count": 5
          },
          {
            "test": "OCR Context Recall",
            "description": "Tests retrieval of previous OCR fact-checking analyses",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 1.0,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What did we find about vaccine misinformation earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Image Context",
            "description": "Tests ability to connect related misinformation across multiple images",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.7595616042892774,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Previous climate misinformation we analyzed",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Similar Content Disambiguation",
            "description": "Tests ability to distinguish between very similar statements",
            "metrics": {
              "precision": 0.3333333333333333,
              "recall": 1.0,
              "f1_score": 0.5,
              "relevance_score": 1.0,
              "retrieved_count": 3,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was said about carbon emissions during plant operation specifically?",
            "expected_count": 1,
            "retrieved_count": 3
          },
          {
            "test": "Long-Term Memory Retention",
            "description": "Tests retrieval of early content after many subsequent memories",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 1.0,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was the opening statement about solar energy?",
            "expected_count": 1,
            "retrieved_count": 5
          }
        ],
        "metadata": {
          "execution_time": 0.919869,
          "start_time": "2025-11-11T17:54:42.920819",
          "end_time": "2025-11-11T17:54:43.840688",
          "memory_backend": "unknown",
          "embedding_model": "unknown"
        }
      },
      "total_time": 0.9385185241699219,
      "timestamp": "2025-11-11 17:54:43",
      "implementation_status": "baseline_only"
    },
    {
      "percentile": 0.75,
      "results": {
        "summary": {
          "total_tests": 13,
          "passed": 5,
          "pass_rate": 0.38461538461538464,
          "avg_precision": 0.3294871794871795,
          "avg_recall": 0.9230769230769231,
          "avg_f1": 0.4750915750915751,
          "avg_relevance_score": 0.7024616869818677,
          "target_met": false
        },
        "individual_results": [
          {
            "test": "Exact Turn Recall",
            "description": "Tests ability to retrieve exact turn content when explicitly requested",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.6037754535563129,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did the proponent say about safety in turn 1?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic-Based Retrieval",
            "description": "Tests semantic search for topic-related arguments across multiple turns",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5942560801765377,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Arguments about economic costs and affordability",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Filtering",
            "description": "Tests ability to filter memories by role metadata",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6274056261270253,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What environmental arguments did the opponent make?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Recent Context Retrieval",
            "description": "Tests ability to prioritize recent context over older content",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.9362661121722125,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": true,
            "query": "Most recent arguments in the debate about modern technology",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Irrelevant Query Handling",
            "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
            "metrics": {
              "precision": 0.0,
              "recall": 0.0,
              "f1_score": 0.0,
              "relevance_score": 0.0,
              "retrieved_count": 2,
              "expected_count": 0,
              "true_positives": 0
            },
            "passed": false,
            "query": "What was said about cryptocurrency mining and blockchain technology?",
            "expected_count": 0,
            "retrieved_count": 2
          },
          {
            "test": "Role Reversal - Original Stance Retrieval",
            "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5134742766974831,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What was my original argument FOR solar energy that I now need to critique?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Reversal - Adopt Opponent's Position",
            "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5911535010563447,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What arguments against nuclear energy should I now adopt after role reversal?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Turn Chat Context",
            "description": "Tests retrieval of previous explanations in multi-turn conversation",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.5907575156397695,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did you explain about entanglement earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic Switching",
            "description": "Tests ability to retrieve correct topic context after switching topics",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 0.9159689152581154,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "Previous discussion about AI safety concerns and alignment",
            "expected_count": 1,
            "retrieved_count": 5
          },
          {
            "test": "OCR Context Recall",
            "description": "Tests retrieval of previous OCR fact-checking analyses",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 1.0,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What did we find about vaccine misinformation earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Image Context",
            "description": "Tests ability to connect related misinformation across multiple images",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.7589444500804794,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Previous climate misinformation we analyzed",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Similar Content Disambiguation",
            "description": "Tests ability to distinguish between very similar statements",
            "metrics": {
              "precision": 0.3333333333333333,
              "recall": 1.0,
              "f1_score": 0.5,
              "relevance_score": 1.0,
              "retrieved_count": 3,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was said about carbon emissions during plant operation specifically?",
            "expected_count": 1,
            "retrieved_count": 3
          },
          {
            "test": "Long-Term Memory Retention",
            "description": "Tests retrieval of early content after many subsequent memories",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 1.0,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was the opening statement about solar energy?",
            "expected_count": 1,
            "retrieved_count": 5
          }
        ],
        "metadata": {
          "execution_time": 0.936992,
          "start_time": "2025-11-11T17:54:45.860605",
          "end_time": "2025-11-11T17:54:46.797597",
          "memory_backend": "unknown",
          "embedding_model": "unknown"
        }
      },
      "total_time": 0.956505537033081,
      "timestamp": "2025-11-11 17:54:46",
      "implementation_status": "baseline_only"
    },
    {
      "percentile": 0.8,
      "results": {
        "summary": {
          "total_tests": 13,
          "passed": 5,
          "pass_rate": 0.38461538461538464,
          "avg_precision": 0.3294871794871795,
          "avg_recall": 0.9230769230769231,
          "avg_f1": 0.4750915750915751,
          "avg_relevance_score": 0.7024616869818677,
          "target_met": false
        },
        "individual_results": [
          {
            "test": "Exact Turn Recall",
            "description": "Tests ability to retrieve exact turn content when explicitly requested",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.6037754535563129,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did the proponent say about safety in turn 1?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic-Based Retrieval",
            "description": "Tests semantic search for topic-related arguments across multiple turns",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5942560801765377,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Arguments about economic costs and affordability",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Filtering",
            "description": "Tests ability to filter memories by role metadata",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6274056261270253,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What environmental arguments did the opponent make?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Recent Context Retrieval",
            "description": "Tests ability to prioritize recent context over older content",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.9362661121722125,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": true,
            "query": "Most recent arguments in the debate about modern technology",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Irrelevant Query Handling",
            "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
            "metrics": {
              "precision": 0.0,
              "recall": 0.0,
              "f1_score": 0.0,
              "relevance_score": 0.0,
              "retrieved_count": 2,
              "expected_count": 0,
              "true_positives": 0
            },
            "passed": false,
            "query": "What was said about cryptocurrency mining and blockchain technology?",
            "expected_count": 0,
            "retrieved_count": 2
          },
          {
            "test": "Role Reversal - Original Stance Retrieval",
            "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5134742766974831,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What was my original argument FOR solar energy that I now need to critique?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Reversal - Adopt Opponent's Position",
            "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5911535010563447,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What arguments against nuclear energy should I now adopt after role reversal?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Turn Chat Context",
            "description": "Tests retrieval of previous explanations in multi-turn conversation",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.5907575156397695,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did you explain about entanglement earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic Switching",
            "description": "Tests ability to retrieve correct topic context after switching topics",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 0.9159689152581154,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "Previous discussion about AI safety concerns and alignment",
            "expected_count": 1,
            "retrieved_count": 5
          },
          {
            "test": "OCR Context Recall",
            "description": "Tests retrieval of previous OCR fact-checking analyses",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 1.0,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What did we find about vaccine misinformation earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Image Context",
            "description": "Tests ability to connect related misinformation across multiple images",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.7589444500804794,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Previous climate misinformation we analyzed",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Similar Content Disambiguation",
            "description": "Tests ability to distinguish between very similar statements",
            "metrics": {
              "precision": 0.3333333333333333,
              "recall": 1.0,
              "f1_score": 0.5,
              "relevance_score": 1.0,
              "retrieved_count": 3,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was said about carbon emissions during plant operation specifically?",
            "expected_count": 1,
            "retrieved_count": 3
          },
          {
            "test": "Long-Term Memory Retention",
            "description": "Tests retrieval of early content after many subsequent memories",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 1.0,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was the opening statement about solar energy?",
            "expected_count": 1,
            "retrieved_count": 5
          }
        ],
        "metadata": {
          "execution_time": 1.141975,
          "start_time": "2025-11-11T17:54:48.819537",
          "end_time": "2025-11-11T17:54:49.961512",
          "memory_backend": "unknown",
          "embedding_model": "unknown"
        }
      },
      "total_time": 1.1613764762878418,
      "timestamp": "2025-11-11 17:54:49",
      "implementation_status": "baseline_only"
    },
    {
      "percentile": 0.85,
      "results": {
        "summary": {
          "total_tests": 13,
          "passed": 5,
          "pass_rate": 0.38461538461538464,
          "avg_precision": 0.3294871794871795,
          "avg_recall": 0.9230769230769231,
          "avg_f1": 0.4750915750915751,
          "avg_relevance_score": 0.7024616869818677,
          "target_met": false
        },
        "individual_results": [
          {
            "test": "Exact Turn Recall",
            "description": "Tests ability to retrieve exact turn content when explicitly requested",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.6037754535563129,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did the proponent say about safety in turn 1?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic-Based Retrieval",
            "description": "Tests semantic search for topic-related arguments across multiple turns",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5942560801765377,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Arguments about economic costs and affordability",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Filtering",
            "description": "Tests ability to filter memories by role metadata",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6274056261270253,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What environmental arguments did the opponent make?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Recent Context Retrieval",
            "description": "Tests ability to prioritize recent context over older content",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.9362661121722125,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": true,
            "query": "Most recent arguments in the debate about modern technology",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Irrelevant Query Handling",
            "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
            "metrics": {
              "precision": 0.0,
              "recall": 0.0,
              "f1_score": 0.0,
              "relevance_score": 0.0,
              "retrieved_count": 2,
              "expected_count": 0,
              "true_positives": 0
            },
            "passed": false,
            "query": "What was said about cryptocurrency mining and blockchain technology?",
            "expected_count": 0,
            "retrieved_count": 2
          },
          {
            "test": "Role Reversal - Original Stance Retrieval",
            "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5134742766974831,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What was my original argument FOR solar energy that I now need to critique?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Reversal - Adopt Opponent's Position",
            "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5911535010563447,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What arguments against nuclear energy should I now adopt after role reversal?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Turn Chat Context",
            "description": "Tests retrieval of previous explanations in multi-turn conversation",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.5907575156397695,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did you explain about entanglement earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic Switching",
            "description": "Tests ability to retrieve correct topic context after switching topics",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 0.9159689152581154,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "Previous discussion about AI safety concerns and alignment",
            "expected_count": 1,
            "retrieved_count": 5
          },
          {
            "test": "OCR Context Recall",
            "description": "Tests retrieval of previous OCR fact-checking analyses",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 1.0,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What did we find about vaccine misinformation earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Image Context",
            "description": "Tests ability to connect related misinformation across multiple images",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.7589444500804794,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Previous climate misinformation we analyzed",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Similar Content Disambiguation",
            "description": "Tests ability to distinguish between very similar statements",
            "metrics": {
              "precision": 0.3333333333333333,
              "recall": 1.0,
              "f1_score": 0.5,
              "relevance_score": 1.0,
              "retrieved_count": 3,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was said about carbon emissions during plant operation specifically?",
            "expected_count": 1,
            "retrieved_count": 3
          },
          {
            "test": "Long-Term Memory Retention",
            "description": "Tests retrieval of early content after many subsequent memories",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 1.0,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was the opening statement about solar energy?",
            "expected_count": 1,
            "retrieved_count": 5
          }
        ],
        "metadata": {
          "execution_time": 1.961267,
          "start_time": "2025-11-11T17:54:51.977113",
          "end_time": "2025-11-11T17:54:53.938380",
          "memory_backend": "unknown",
          "embedding_model": "unknown"
        }
      },
      "total_time": 1.9900588989257812,
      "timestamp": "2025-11-11 17:54:53",
      "implementation_status": "baseline_only"
    },
    {
      "percentile": 0.9,
      "results": {
        "summary": {
          "total_tests": 13,
          "passed": 5,
          "pass_rate": 0.38461538461538464,
          "avg_precision": 0.3294871794871795,
          "avg_recall": 0.9230769230769231,
          "avg_f1": 0.4750915750915751,
          "avg_relevance_score": 0.7024616869818677,
          "target_met": false
        },
        "individual_results": [
          {
            "test": "Exact Turn Recall",
            "description": "Tests ability to retrieve exact turn content when explicitly requested",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.6037754535563129,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did the proponent say about safety in turn 1?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic-Based Retrieval",
            "description": "Tests semantic search for topic-related arguments across multiple turns",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5942560801765377,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Arguments about economic costs and affordability",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Filtering",
            "description": "Tests ability to filter memories by role metadata",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.6274056261270253,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What environmental arguments did the opponent make?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Recent Context Retrieval",
            "description": "Tests ability to prioritize recent context over older content",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.9362661121722125,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": true,
            "query": "Most recent arguments in the debate about modern technology",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Irrelevant Query Handling",
            "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
            "metrics": {
              "precision": 0.0,
              "recall": 0.0,
              "f1_score": 0.0,
              "relevance_score": 0.0,
              "retrieved_count": 2,
              "expected_count": 0,
              "true_positives": 0
            },
            "passed": false,
            "query": "What was said about cryptocurrency mining and blockchain technology?",
            "expected_count": 0,
            "retrieved_count": 2
          },
          {
            "test": "Role Reversal - Original Stance Retrieval",
            "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5134742766974831,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What was my original argument FOR solar energy that I now need to critique?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Role Reversal - Adopt Opponent's Position",
            "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
            "metrics": {
              "precision": 0.5,
              "recall": 1.0,
              "f1_score": 0.6666666666666666,
              "relevance_score": 0.5911535010563447,
              "retrieved_count": 4,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "What arguments against nuclear energy should I now adopt after role reversal?",
            "expected_count": 2,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Turn Chat Context",
            "description": "Tests retrieval of previous explanations in multi-turn conversation",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 0.5907575156397695,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": false,
            "query": "What did you explain about entanglement earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Topic Switching",
            "description": "Tests ability to retrieve correct topic context after switching topics",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 0.9159689152581154,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "Previous discussion about AI safety concerns and alignment",
            "expected_count": 1,
            "retrieved_count": 5
          },
          {
            "test": "OCR Context Recall",
            "description": "Tests retrieval of previous OCR fact-checking analyses",
            "metrics": {
              "precision": 0.25,
              "recall": 1.0,
              "f1_score": 0.4,
              "relevance_score": 1.0,
              "retrieved_count": 4,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What did we find about vaccine misinformation earlier?",
            "expected_count": 1,
            "retrieved_count": 4
          },
          {
            "test": "Multi-Image Context",
            "description": "Tests ability to connect related misinformation across multiple images",
            "metrics": {
              "precision": 0.4,
              "recall": 1.0,
              "f1_score": 0.5714285714285715,
              "relevance_score": 0.7589444500804794,
              "retrieved_count": 5,
              "expected_count": 2,
              "true_positives": 2
            },
            "passed": false,
            "query": "Previous climate misinformation we analyzed",
            "expected_count": 2,
            "retrieved_count": 5
          },
          {
            "test": "Similar Content Disambiguation",
            "description": "Tests ability to distinguish between very similar statements",
            "metrics": {
              "precision": 0.3333333333333333,
              "recall": 1.0,
              "f1_score": 0.5,
              "relevance_score": 1.0,
              "retrieved_count": 3,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was said about carbon emissions during plant operation specifically?",
            "expected_count": 1,
            "retrieved_count": 3
          },
          {
            "test": "Long-Term Memory Retention",
            "description": "Tests retrieval of early content after many subsequent memories",
            "metrics": {
              "precision": 0.2,
              "recall": 1.0,
              "f1_score": 0.33333333333333337,
              "relevance_score": 1.0,
              "retrieved_count": 5,
              "expected_count": 1,
              "true_positives": 1
            },
            "passed": true,
            "query": "What was the opening statement about solar energy?",
            "expected_count": 1,
            "retrieved_count": 5
          }
        ],
        "metadata": {
          "execution_time": 0.764326,
          "start_time": "2025-11-11T17:54:55.967714",
          "end_time": "2025-11-11T17:54:56.732040",
          "memory_backend": "unknown",
          "embedding_model": "unknown"
        }
      },
      "total_time": 0.7818818092346191,
      "timestamp": "2025-11-11 17:54:56",
      "implementation_status": "baseline_only"
    }
  ],
  "comparison": {
    "best_precision": {
      "percentile": 0.6,
      "precision": 32.94871794871795,
      "relevance": 74.78316897328251,
      "recall": 92.3076923076923,
      "delta_precision": -0.001282051282053942,
      "combined_score": 49.68249835854378
    },
    "best_combined": {
      "percentile": 0.6,
      "precision": 32.94871794871795,
      "relevance": 74.78316897328251,
      "recall": 92.3076923076923,
      "delta_precision": -0.001282051282053942,
      "delta_relevance": 0.003168973282512866,
      "combined_score": 49.68249835854378
    },
    "meets_criteria": false
  },
  "success_criteria": {
    "min_precision_gain": 0.5,
    "min_relevance": 74.5,
    "min_recall": 90.0,
    "max_latency_ms": 80
  },
  "baseline_metrics": {
    "relevance": 74.78,
    "precision": 32.95,
    "recall": 92.31
  }
}