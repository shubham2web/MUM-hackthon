[
  {
    "strategy": "9g-0-baseline",
    "description": "No expansion (control)",
    "results": {
      "summary": {
        "total_tests": 13,
        "passed": 6,
        "pass_rate": 0.46153846153846156,
        "avg_precision": 0.3294871794871795,
        "avg_recall": 0.9230769230769231,
        "avg_f1": 0.4750915750915751,
        "avg_relevance_score": 0.7478316897328251,
        "target_met": false
      },
      "individual_results": [
        {
          "test": "Exact Turn Recall",
          "description": "Tests ability to retrieve exact turn content when explicitly requested",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.6925304452290909,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did the proponent say about safety in turn 1?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic-Based Retrieval",
          "description": "Tests semantic search for topic-related arguments across multiple turns",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.6776253042486277,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Arguments about economic costs and affordability",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Filtering",
          "description": "Tests ability to filter memories by role metadata",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.7111510428902025,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What environmental arguments did the opponent make?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Recent Context Retrieval",
          "description": "Tests ability to prioritize recent context over older content",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.9905812431376531,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": true,
          "query": "Most recent arguments in the debate about modern technology",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Irrelevant Query Handling",
          "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
          "metrics": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "relevance_score": 0.0,
            "retrieved_count": 2,
            "expected_count": 0,
            "true_positives": 0
          },
          "passed": false,
          "query": "What was said about cryptocurrency mining and blockchain technology?",
          "expected_count": 0,
          "retrieved_count": 2
        },
        {
          "test": "Role Reversal - Original Stance Retrieval",
          "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5150507670710887,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What was my original argument FOR solar energy that I now need to critique?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Reversal - Adopt Opponent's Position",
          "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.6572698805898227,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What arguments against nuclear energy should I now adopt after role reversal?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Turn Chat Context",
          "description": "Tests retrieval of previous explanations in multi-turn conversation",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.653673191055405,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did you explain about entanglement earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic Switching",
          "description": "Tests ability to retrieve correct topic context after switching topics",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 1.0,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "Previous discussion about AI safety concerns and alignment",
          "expected_count": 1,
          "retrieved_count": 5
        },
        {
          "test": "OCR Context Recall",
          "description": "Tests retrieval of previous OCR fact-checking analyses",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 1.0,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What did we find about vaccine misinformation earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Image Context",
          "description": "Tests ability to connect related misinformation across multiple images",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.8239300923048358,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": true,
          "query": "Previous climate misinformation we analyzed",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Similar Content Disambiguation",
          "description": "Tests ability to distinguish between very similar statements",
          "metrics": {
            "precision": 0.3333333333333333,
            "recall": 1.0,
            "f1_score": 0.5,
            "relevance_score": 1.0,
            "retrieved_count": 3,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was said about carbon emissions during plant operation specifically?",
          "expected_count": 1,
          "retrieved_count": 3
        },
        {
          "test": "Long-Term Memory Retention",
          "description": "Tests retrieval of early content after many subsequent memories",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 1.0,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was the opening statement about solar energy?",
          "expected_count": 1,
          "retrieved_count": 5
        }
      ]
    }
  },
  {
    "strategy": "9g-1-synonyms",
    "description": "Add common synonyms",
    "results": {
      "summary": {
        "total_tests": 13,
        "passed": 5,
        "pass_rate": 0.38461538461538464,
        "avg_precision": 0.3294871794871795,
        "avg_recall": 0.9230769230769231,
        "avg_f1": 0.4750915750915751,
        "avg_relevance_score": 0.6915272960469647,
        "target_met": false
      },
      "individual_results": [
        {
          "test": "Exact Turn Recall",
          "description": "Tests ability to retrieve exact turn content when explicitly requested",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.5996155307061044,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did the proponent say about safety in turn 1?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic-Based Retrieval",
          "description": "Tests semantic search for topic-related arguments across multiple turns",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.6137868795507953,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Arguments about economic costs and affordability",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Filtering",
          "description": "Tests ability to filter memories by role metadata",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.6586177146481444,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What environmental arguments did the opponent make?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Recent Context Retrieval",
          "description": "Tests ability to prioritize recent context over older content",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.9374102965698405,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": true,
          "query": "Most recent arguments in the debate about modern technology",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Irrelevant Query Handling",
          "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
          "metrics": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "relevance_score": 0.0,
            "retrieved_count": 2,
            "expected_count": 0,
            "true_positives": 0
          },
          "passed": false,
          "query": "What was said about cryptocurrency mining and blockchain technology?",
          "expected_count": 0,
          "retrieved_count": 2
        },
        {
          "test": "Role Reversal - Original Stance Retrieval",
          "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.4056757744221778,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What was my original argument FOR solar energy that I now need to critique?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Reversal - Adopt Opponent's Position",
          "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5458148681142829,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What arguments against nuclear energy should I now adopt after role reversal?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Turn Chat Context",
          "description": "Tests retrieval of previous explanations in multi-turn conversation",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.6031634234682046,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did you explain about entanglement earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic Switching",
          "description": "Tests ability to retrieve correct topic context after switching topics",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 0.9745950460186117,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "Previous discussion about AI safety concerns and alignment",
          "expected_count": 1,
          "retrieved_count": 5
        },
        {
          "test": "OCR Context Recall",
          "description": "Tests retrieval of previous OCR fact-checking analyses",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 1.0,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What did we find about vaccine misinformation earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Image Context",
          "description": "Tests ability to connect related misinformation across multiple images",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.651175315112379,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Previous climate misinformation we analyzed",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Similar Content Disambiguation",
          "description": "Tests ability to distinguish between very similar statements",
          "metrics": {
            "precision": 0.3333333333333333,
            "recall": 1.0,
            "f1_score": 0.5,
            "relevance_score": 1.0,
            "retrieved_count": 3,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was said about carbon emissions during plant operation specifically?",
          "expected_count": 1,
          "retrieved_count": 3
        },
        {
          "test": "Long-Term Memory Retention",
          "description": "Tests retrieval of early content after many subsequent memories",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 1.0,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was the opening statement about solar energy?",
          "expected_count": 1,
          "retrieved_count": 5
        }
      ]
    }
  },
  {
    "strategy": "9g-2-entities",
    "description": "Emphasize key entities",
    "results": {
      "summary": {
        "total_tests": 13,
        "passed": 5,
        "pass_rate": 0.38461538461538464,
        "avg_precision": 0.3294871794871795,
        "avg_recall": 0.9230769230769231,
        "avg_f1": 0.4750915750915751,
        "avg_relevance_score": 0.692961159439647,
        "target_met": false
      },
      "individual_results": [
        {
          "test": "Exact Turn Recall",
          "description": "Tests ability to retrieve exact turn content when explicitly requested",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.5960696927493392,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did the proponent say about safety in turn 1?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic-Based Retrieval",
          "description": "Tests semantic search for topic-related arguments across multiple turns",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5952639254936487,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Arguments about economic costs and affordability",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Filtering",
          "description": "Tests ability to filter memories by role metadata",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.6267713358927616,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What environmental arguments did the opponent make?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Recent Context Retrieval",
          "description": "Tests ability to prioritize recent context over older content",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.9211468420286263,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": true,
          "query": "Most recent arguments in the debate about modern technology",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Irrelevant Query Handling",
          "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
          "metrics": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "relevance_score": 0.0,
            "retrieved_count": 2,
            "expected_count": 0,
            "true_positives": 0
          },
          "passed": false,
          "query": "What was said about cryptocurrency mining and blockchain technology?",
          "expected_count": 0,
          "retrieved_count": 2
        },
        {
          "test": "Role Reversal - Original Stance Retrieval",
          "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.47397348967193265,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What was my original argument FOR solar energy that I now need to critique?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Reversal - Adopt Opponent's Position",
          "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.593713506962824,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What arguments against nuclear energy should I now adopt after role reversal?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Turn Chat Context",
          "description": "Tests retrieval of previous explanations in multi-turn conversation",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.5834277036843555,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did you explain about entanglement earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic Switching",
          "description": "Tests ability to retrieve correct topic context after switching topics",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 0.884491640874405,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "Previous discussion about AI safety concerns and alignment",
          "expected_count": 1,
          "retrieved_count": 5
        },
        {
          "test": "OCR Context Recall",
          "description": "Tests retrieval of previous OCR fact-checking analyses",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 1.0,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What did we find about vaccine misinformation earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Image Context",
          "description": "Tests ability to connect related misinformation across multiple images",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.7336369353575183,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Previous climate misinformation we analyzed",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Similar Content Disambiguation",
          "description": "Tests ability to distinguish between very similar statements",
          "metrics": {
            "precision": 0.3333333333333333,
            "recall": 1.0,
            "f1_score": 0.5,
            "relevance_score": 1.0,
            "retrieved_count": 3,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was said about carbon emissions during plant operation specifically?",
          "expected_count": 1,
          "retrieved_count": 3
        },
        {
          "test": "Long-Term Memory Retention",
          "description": "Tests retrieval of early content after many subsequent memories",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 1.0,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was the opening statement about solar energy?",
          "expected_count": 1,
          "retrieved_count": 5
        }
      ]
    }
  },
  {
    "strategy": "9g-3-keywords",
    "description": "Add domain context",
    "results": {
      "summary": {
        "total_tests": 13,
        "passed": 4,
        "pass_rate": 0.3076923076923077,
        "avg_precision": 0.3294871794871795,
        "avg_recall": 0.9230769230769231,
        "avg_f1": 0.4750915750915751,
        "avg_relevance_score": 0.6767022326692628,
        "target_met": false
      },
      "individual_results": [
        {
          "test": "Exact Turn Recall",
          "description": "Tests ability to retrieve exact turn content when explicitly requested",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.6050932055872282,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did the proponent say about safety in turn 1?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic-Based Retrieval",
          "description": "Tests semantic search for topic-related arguments across multiple turns",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5629731892998683,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Arguments about economic costs and affordability",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Filtering",
          "description": "Tests ability to filter memories by role metadata",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.7289370028908846,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What environmental arguments did the opponent make?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Recent Context Retrieval",
          "description": "Tests ability to prioritize recent context over older content",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.7385747399265654,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Most recent arguments in the debate about modern technology",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Irrelevant Query Handling",
          "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
          "metrics": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "relevance_score": 0.0,
            "retrieved_count": 2,
            "expected_count": 0,
            "true_positives": 0
          },
          "passed": false,
          "query": "What was said about cryptocurrency mining and blockchain technology?",
          "expected_count": 0,
          "retrieved_count": 2
        },
        {
          "test": "Role Reversal - Original Stance Retrieval",
          "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.38232990084491525,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What was my original argument FOR solar energy that I now need to critique?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Reversal - Adopt Opponent's Position",
          "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5316947926944685,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What arguments against nuclear energy should I now adopt after role reversal?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Turn Chat Context",
          "description": "Tests retrieval of previous explanations in multi-turn conversation",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.6286763952859543,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did you explain about entanglement earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic Switching",
          "description": "Tests ability to retrieve correct topic context after switching topics",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 0.859905348090052,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "Previous discussion about AI safety concerns and alignment",
          "expected_count": 1,
          "retrieved_count": 5
        },
        {
          "test": "OCR Context Recall",
          "description": "Tests retrieval of previous OCR fact-checking analyses",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 1.0,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What did we find about vaccine misinformation earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Image Context",
          "description": "Tests ability to connect related misinformation across multiple images",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.7589444500804794,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Previous climate misinformation we analyzed",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Similar Content Disambiguation",
          "description": "Tests ability to distinguish between very similar statements",
          "metrics": {
            "precision": 0.3333333333333333,
            "recall": 1.0,
            "f1_score": 0.5,
            "relevance_score": 1.0,
            "retrieved_count": 3,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was said about carbon emissions during plant operation specifically?",
          "expected_count": 1,
          "retrieved_count": 3
        },
        {
          "test": "Long-Term Memory Retention",
          "description": "Tests retrieval of early content after many subsequent memories",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 1.0,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was the opening statement about solar energy?",
          "expected_count": 1,
          "retrieved_count": 5
        }
      ]
    }
  },
  {
    "strategy": "9g-4-reformulate",
    "description": "Question \u2192 Statement",
    "results": {
      "summary": {
        "total_tests": 13,
        "passed": 5,
        "pass_rate": 0.38461538461538464,
        "avg_precision": 0.3294871794871795,
        "avg_recall": 0.9230769230769231,
        "avg_f1": 0.4750915750915751,
        "avg_relevance_score": 0.7037375085978119,
        "target_met": false
      },
      "individual_results": [
        {
          "test": "Exact Turn Recall",
          "description": "Tests ability to retrieve exact turn content when explicitly requested",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.5794974060251894,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did the proponent say about safety in turn 1?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic-Based Retrieval",
          "description": "Tests semantic search for topic-related arguments across multiple turns",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5942560801765377,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Arguments about economic costs and affordability",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Filtering",
          "description": "Tests ability to filter memories by role metadata",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.6274056261270253,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What environmental arguments did the opponent make?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Recent Context Retrieval",
          "description": "Tests ability to prioritize recent context over older content",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.9362661121722125,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": true,
          "query": "Most recent arguments in the debate about modern technology",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Irrelevant Query Handling",
          "description": "Tests that system doesn't hallucinate or return false positives for irrelevant queries",
          "metrics": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "relevance_score": 0.0,
            "retrieved_count": 2,
            "expected_count": 0,
            "true_positives": 0
          },
          "passed": false,
          "query": "What was said about cryptocurrency mining and blockchain technology?",
          "expected_count": 0,
          "retrieved_count": 2
        },
        {
          "test": "Role Reversal - Original Stance Retrieval",
          "description": "CRITICAL: Tests if reversed agent can retrieve their original position to argue against it",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5134742766974831,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What was my original argument FOR solar energy that I now need to critique?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Role Reversal - Adopt Opponent's Position",
          "description": "Tests if reversed agent can adopt the opponent's previous arguments as their own",
          "metrics": {
            "precision": 0.5,
            "recall": 1.0,
            "f1_score": 0.6666666666666666,
            "relevance_score": 0.5911535010563447,
            "retrieved_count": 4,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "What arguments against nuclear energy should I now adopt after role reversal?",
          "expected_count": 2,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Turn Chat Context",
          "description": "Tests retrieval of previous explanations in multi-turn conversation",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 0.6259689821727844,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": false,
          "query": "What did you explain about entanglement earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Topic Switching",
          "description": "Tests ability to retrieve correct topic context after switching topics",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 1.0,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "Previous discussion about AI safety concerns and alignment",
          "expected_count": 1,
          "retrieved_count": 5
        },
        {
          "test": "OCR Context Recall",
          "description": "Tests retrieval of previous OCR fact-checking analyses",
          "metrics": {
            "precision": 0.25,
            "recall": 1.0,
            "f1_score": 0.4,
            "relevance_score": 1.0,
            "retrieved_count": 4,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What did we find about vaccine misinformation earlier?",
          "expected_count": 1,
          "retrieved_count": 4
        },
        {
          "test": "Multi-Image Context",
          "description": "Tests ability to connect related misinformation across multiple images",
          "metrics": {
            "precision": 0.4,
            "recall": 1.0,
            "f1_score": 0.5714285714285715,
            "relevance_score": 0.680565627343978,
            "retrieved_count": 5,
            "expected_count": 2,
            "true_positives": 2
          },
          "passed": false,
          "query": "Previous climate misinformation we analyzed",
          "expected_count": 2,
          "retrieved_count": 5
        },
        {
          "test": "Similar Content Disambiguation",
          "description": "Tests ability to distinguish between very similar statements",
          "metrics": {
            "precision": 0.3333333333333333,
            "recall": 1.0,
            "f1_score": 0.5,
            "relevance_score": 1.0,
            "retrieved_count": 3,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was said about carbon emissions during plant operation specifically?",
          "expected_count": 1,
          "retrieved_count": 3
        },
        {
          "test": "Long-Term Memory Retention",
          "description": "Tests retrieval of early content after many subsequent memories",
          "metrics": {
            "precision": 0.2,
            "recall": 1.0,
            "f1_score": 0.33333333333333337,
            "relevance_score": 1.0,
            "retrieved_count": 5,
            "expected_count": 1,
            "true_positives": 1
          },
          "passed": true,
          "query": "What was the opening statement about solar energy?",
          "expected_count": 1,
          "retrieved_count": 5
        }
      ]
    }
  }
]